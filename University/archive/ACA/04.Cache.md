Most important problem in computer architecture is the gap between CPU speed and external memory speed

![](https://i.imgur.com/8be5kf5.png)

Usually every 1.5 year there is a 2x improvement in CPU speed but there is an 9% improvement in memory speed(almost constant). This could lead to a huge bottleneck. Cannot make DRAM much faster so the solution is hide this latency(Use small fast cache memories between CPU and DRAM).
Can have multiple request per clock-cycle from multi-threading, etc...(CPU arrives at more or less 409 Gb/s DRAM to 25 Gb/s). One possible solution is to optimise the access to DRAM providing multi-port system(request in parallel). Problem from multiple ports is that there could be conflicts, accesses have to be consistent. Can create different spaces but then you have to handle the partition of the data(accesses have to be handled correctly and could request the same data). Single accesses could lead to problem(guarantee WaR). Can add multiple level of cache, hiding the latency of the system, can use a third level shared at chip level. Power consumption goes up and this is a recurrent problem in architecture creation. Needs to handle it. Goal in memory hierarchy is to have the memory to seem large, fast and cheap. Want to officially access data on DRAM level but with the speed access of a register. Memory hierarchy is the combination of different modules with different cost and size and different access mechanism. 

![500](https://i.imgur.com/Zekcg7C.png)

### Principle of locality:
Programs access a relatively small portion of the address space at any instant of time. Instead of transferring the single data we transfer the entire block on the cache so the next access to it will be faster. If the cache is faster is true to access to all the consecutive blocks, but need to pay the first access cost.
 Two predictable properties of memory references:
- Temporal Locality: If a location is referenced, it is likely to be referenced again in the near future (e.g., loops, reuse).
- Spatial Locality: If a location is referenced it is likely that locations near it will be referenced in the near future (e.g., straightline code, array access). HW based on this for the past 15 years.
When I move to another block I pay the cost of DRAM. The more an application is data intensive the more these principles are usable. If I know how the application is structured I can organize the data to use the Spatial Locality principle even more.

![450](https://i.imgur.com/3iZU5KL.png)

# Caches

![400](https://i.imgur.com/O3VM3tM.png)

Cache exploits temporal locality and spatial locality fetching the bloc. The idea is we look at the block the process want to access to as it is written in memory. Then we can look in the cache to see if the data are already inside the cache. If there is an hit we can return the copy in the cache.  Otherwise you have to look in the DRAM. IN the beginning you are filling the cache with the miss. After a number of miss the cache will contain the data you are searching and the response will be faster. A perfect system has a high Hit Rate(Hit Rate cannot be 1 as it is needed at least one access to the memory and its related miss). Miss Rate is how much we have miss. We want to increase the Hit Rate as much as possible. Hit time is the time needed to access data in cache. Miss Time is the time needed to transfer the data to the cache after a miss.

![](https://i.imgur.com/JHC9BgU.png)

#### Where can a block be placed in the cache?
Need some sort of organization otherwise the checks are on all the data stored in the cache increasing the time needed to check. Depends on the type of memory implemented: memory block that goes to the same block based on the address. Example is giving the block based on the result of module 8(easy system). The handling could result in conflicts. Another solution is the block goes anywhere.
Set Associative: solution in the middle where I divide the cache in groups and a block can goes ONLY in a specific group where it ca occupy any location. This is useful as I then only check only the location in the group corresponding to the block.

![400](https://i.imgur.com/7sDYnVr.png)

##### Source of cache misses:
- Compulsory: cold start or process migration, first reference, first access to a block
- Capacity: cache cannot contain all blocks accessed by the program
- Conflict: multiple memory location mapped on the same cache location(Solution is to increase the cache size or increase associativity). Need to update the memory if I replace a block in the cache.
- Coherence: other process updates memory
#### How is a block found if it is in the cache?
Based on the block placement in the cache. 
I can decompose my address in a part related on how to identify the block in the cache and a part related on how to identify the value.

![400](https://i.imgur.com/L3F2fvz.png)

![400](https://i.imgur.com/VgDEYrL.png)

![400](https://i.imgur.com/cIutt0k.png)

![400](https://i.imgur.com/0CVqaSR.png)

![400](https://i.imgur.com/8rnRpir.png)

![400](https://i.imgur.com/Td0WTrm.png)

#### Which block should be replaced on a miss?
Easy on direct access
In Set associative or Full Associative I can decide on a random method or I can replace the least recently used for the principle of locality(requires a counter resetted to 0 for every hit on the block increased for every access in memory that doesn't access the block). Can also use a method that replace the oldest block but this could increase the time used as the oldest block could be accessed recently but it doesn't require a counter. 
Replacement policy has a second order effect since replacement only happens on misses. 

![400](https://i.imgur.com/ivIzsYA.png)

I need to value if the LRU solution gives me an increase in the time that is so much bigger of the Random solution to mitigate the cost of creating the LRU solution.
#### What happens on a write?
It is important because the write has to be saved in cache and in the DRAM. The cache is volatile so if the power goes off the computer use some emergency battery power to propagate the data in the cache to the DRAM. Needs to verify if the block is really valid.
I can decide to write on memory after every write in the cache. This cause an increase in the memory access and traffic(write through). I can replace the block in memory only when the block is replaced in cache(write back), this decrease the access in memory. 
I could have a write during cache miss so two cases:
- no write allocate: I only write in the main memory
- write allocate: (fetch on write) I fetch in the cache transferring the block in cache and then doing the write
Common combination:
- write through and no write allocate
- write back with write allocate
