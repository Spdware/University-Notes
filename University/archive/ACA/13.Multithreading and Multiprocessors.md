To improve performance when you cannot do much you can try to replicate the worker.
The difference between multithreading and multiprocessors is important as they are more efficient in different situations. These solutions came in hand when the architectures cannot keep up with the performance required by the programs.

![](https://i.imgur.com/oNM7EKy.png)

The green line is the number of transistor and it should follow Moore Law. But then there are a series of observation as the frequencies(blue) where you have a quadratic increase until 2000s where there is a plateau.
The activities became concetrated creating hot spots and black pots creating problems for dissipations. The idea now is to increase the number of transistors for chip and not for core as you put together more cores increasing the computation capabilities of your processor(simple idea: when the single worker is not enough you put more single worker together, maybe also simplifying the single worker).
Start to the explicit parallelism. 
### Motivations for paradigm change
- Modern processors fail to utilize execution resources well(architectures couldn't execute very well this new paradigm)
- There is no single culprit:
	- Memory conflicts, control hazards, branch misprediction, cache miss….
- Attacking the problems one at a time always has limited effectiveness
- Need for a general latency-tolerance solution which can hide all sources of latency can have a large impact on performance
### Parallel programming
- Explicit parallelism implies structuring the applications into concurrent and communicating tasks. Maybe need a synchronization procedure
- Operating systems offer support for different types of tasks. The most important and frequent are:
	– processes
	– threads

![](https://i.imgur.com/3Mu8kin.png)

![](https://i.imgur.com/ivbwTa2.png)

- The operating systems implement multitasking differently based on the characteristics of the processor:
	– single core
	– single core with multithreading support
	– multicore
### Multithreaded Execution
Multithreading: multiple threads to share the functional units of 1 processor via overlapping
	– processor must duplicate independent state of each thread e.g., a separate copy of register file, a separate PC, and for running independent programs, a separate page table
	– memory shared through the virtual memory mechanisms, which already support multiple processes
	– HW for fast thread switch; much faster than full process switch »100s to 1000s of clocks
- When switch?
	– Alternate instruction per thread (fine grain)
	– When a thread is stalled, perhaps for a cache miss, another thread can be executed (coarse grain)
In these case you are not executing in parallel but the perception is if you complete the first task you also executed the second as you have switched execution in the middle. 
### Thread-level parallelism (TLP)
Fine grained multithreading
	– Switches from one thread to the other at each instruction – the execution of more threads is interleaved (often the switching is performed taking turns, skipping one thread if there is a stall)
	– The CPU must be able to change thread at every clock cycle. It is necessary to duplicated the hardware resources.
Coarse grained multithreading
	– switching from one thread to another occurs only when there are long stalls – e.g., for a miss on the second level cache.
	– Two threads share many system resources (e.g., architectural registers), the switching from one thread to the next requires different clock cycles to save the context
#### Coarse grained multithreading
 Advantage: in normal conditions the single thread is not slowed down
	– Relieves need to have very fast thread-switching
	– Doesn’t slow down thread, since instructions from other threads issued only when the thread encounters a costly stall
Disadvantage: for short stalls it does not reduce the  throughput loss – the CPU starts the execution of instructions that belonged to a single thread, when there is one stall it is necessary to empty the pipeline before starting the new thread
### Do both ILP and TLP
They are not mutual exclusive as ILP is internal to the code while the TLP is intrastructure and is based on multiple pieces of code.
TLP and ILP exploit two different kinds of parallel structure in a program 
Could a processor oriented at ILP to exploit 
TLP? Functional units are often idle in data path designed for ILP because of either stalls or dependencies in the code 
Could the TLP be used as a source of independent instructions that might keep the 
processor busy during stalls? Yes as during stalls you can move in instructions frm other threads
### Thread Level Parallelism Simultaneous Multithreading
- Uses the resources of one superscalar processor to exploit simultaneously ILP and TLP.
- Key motivation: a CPU today has more functional resources than what one thread can in fact use
- Thanks to register renaming and dynamic scheduling, more independent instructions to different threads may be issued without worrying about dependences (that are solved by the hardware of the dynamic scheduling).
- Simultaneously schedule instructions for execution from all threads
You can have register renaming, dynamic scheduling and put together instructions from different threads as the instructions are taken care of at HW level. You can trow in operations from different threads as they are of the same code knowing they are independent. You are executing multiple threads and you are executing them at the same time thanks to the parallelism. 
### Simultaneous Multithreading (SMT) 
- Simultaneous multithreading (SMT): insight that dynamically scheduled processor already has many HW mechanisms to support multithreading
	– Large set of virtual registers that can be used to hold the register sets of independent threads 
	– Register renaming provides unique register identifiers, so instructions from multiple threads can be mixed in datapath without confusing sources and destinations across threads
	– Out-of-order completion allows the threads to execute out of order, and get better utilization of the HW 
- Just adding a per thread renaming table and keeping separate PCs
	– Independent commitment can be supported by logically keeping a separate reorder buffer for each thread
- The system can be dynamically adapted to the environment, allowing (if possible) the execution of instructions from each thread, and allowing that the instructions of a single thread used all functional units if the other thread incurs in a long latency event. 
- More threads use the issues possibilities of the CPU at each cycle; ideally, the exploitation of the issues availabilities is limited only by the unbalance between resources requests and availabilities

![](https://i.imgur.com/26fdb69.png)

![](https://i.imgur.com/jeI4Luc.png)

Difficult to increase performance and clock frequency of the single core
Deep pipeline:
- Heat dissipation problems
- Speed light transmission problems in wires 
- Difficulties in design and verification
- Requirement of very large design groups
Many new applications are multi-threaded
### Beyond ILP
ILP architectures (superscalar, VLIW...):
- Support fine-grained, instruction-level parallelism;
- Fail to support large-scale parallel systems;
Multiple-issue CPUs are very complex, and returns (as far as extracting greater parallelism) are diminishing $\Rightarrow$extracting parallelism at higher levels becomes more and more attractive.
- A further step: process- and thread-level parallel architectures.
- To achieve ever greater performance: connect multiple microprocessors in a complex system

### Parallel Architectures
Definition: “A parallel computer is a collection of processing elements that cooperates and communicate to solve large problems fast”
• Almasi and Gottlieb, Highly Parallel Computing, 1989
The aim is to replicate processors to add performance vs design a faster processor.
- Parallel architecture extends traditional computer architecture with a communication architecture
	- abstractions (HW/SW interface)
	- different structures to realize abstraction efficiently
#### Flynn Taxonomy (1966)
- SISD - Single Instruction Single Data
	- Uniprocessor systems
- MISD - Multiple Instruction Single Data
	- No practical configuration and no commercial systems
- SIMD - Single Instruction Multiple Data
	- Simple programming model, low overhead, flexibility, custom integrated circuits
- MIMD - Multiple Instruction Multiple Data
	- Scalable, fault tolerant, off-the-shelf micros
