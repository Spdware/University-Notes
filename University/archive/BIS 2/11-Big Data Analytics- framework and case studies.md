Closely related to machine learning, artificial intelligence and deep learning...
## Artificial Intelligence vs. Machine Learning
- Artificial intelligence is intelligence demonstrated by machines, in constrast to the natiral intelligence displayed by himand and other animals
- - Machine Learning is a subset of artificial intelligence in the field of computer science that often uses statistical techniques to give computers the ability to learn(i.e. progressively improve performance on a specific task) with data, without being explicitly programmed
So you cannot use them in cases where you cannot have less than 100% reliability and trust as LLMs will give an answer even if they do not know the answer, LLM and errors go hands in hands so we need to think if we can overcome these errors. You can combine different methods to reduce the error on data.
### Artificial Intelligence
- Capabilities generally classified as AI include successfully understanding human speech, competing at the highest level in strategic game systems(such as Go) autonomous cars, intelligent routing in content delivery network and military simulations
- The traditional problems(or goals) of AI research include reasoning, knowledge representation, planning
Having data in your system is a key factor in using artificial intelligence
### Machine Learning
- Unsupervised learning: learning from data without a need for ground truth e.g. clustering or pattern recognition
- Supervised learning: learning from data with ground truth w.g. predictive analytics
Ground truth is data on the true behaviour or status of a system, typically obtained from direct measurement of real-world data. It can be created, also with algorithms, but need correctness in the construction as this will be the foundation of your work.
Topic extraction needs a semantic network to be constructed. Ground truth is expensive to be built as it is manual and for this is also error prone.
### Deep Learning
Class of machine learning algorithms where:
- Use a cascade of multiple layers of nonlinear processing units for feature extraction and transformation, Each successive layer uses the 
#### Natural Language Processing
NLP was performed with semantic(embeds structures of sentences to the words to perform more refined tasks) or syntactic(recognize a string, highly precisional) engines. Problems is that different languages have different structures so a semantic engine is specific for only one type of language. 
Many challenges in NLP: 
- Speech recognition
- Natural Language Understanding
- Natural Language Generation
Still limited in the benefits of talking to a machine instead of typing. Still difficulties in length of the prompt and the costs that they required(energy, infrastructures,...)
## ChatBots
It is a Bot that works for us. It can do work for us that we could do in some clicks. This involves an inversional paradigm of mobile technologies: instead of installing and opening an app we talked to a chatbot so if a company wants to provide us with services through the chatbot they would become invisible as the chatbot will not promote your brand but only give out the services.
Company are questioning if LLMs are enterprise ready. Nowadays we can integrate LLMs in enterprise world with APIs, but there are some barriers:
- Allucinations: LLMs **always** provide an answer, if they do not have the answer to the question they will invent it. Depending on the training set they could also know wrong answers. LLMs learn by whatever content that is provided to them, no filter to their learning process so they can deteriorate. Can be still used if the answer doesn't need to be always correct/errors can be coped. Data science want to provide a consistent story and do not care too much of the correctness of the story(wrong by scientific means, but it is not a wrong process as can help to hide some errors/need to have someone that contradict you). Power BI has LLMs integrated in the backend, not exposed to the user yet.
- Need to extract data from large datasets. If you want to extract data from the original dataset you can upload it to the LLMs, but if these data are an asset of the company and do not want to let them leave your corporation you need to be protected by data theft from LLMs. Do not know how much errors on data analytics the LLMs do. Simple data acquisition is good to do, but too complex can be detrimental to the objective of learning from data(Example LLMs still cannot replace coding, still need good developers).
As LLMs have replaced web searches these give them an edge to the distribution of information. 
## Machine Learning application priority(by industry)
![](https://i.imgur.com/1MXzU75.png)

Customer segmentation is still done by people, not using data analytics to confirm the experience as LLMs are not mature enough for the task.
Changing market segmentation to search for a better revenue, when you are in a good situation, is not really a good idea as it is difficult/expensive to change my market segmentation. Changes take a lot of time to be enacted. 
LLMs are far away from doing data science, there are ML application to do data analysis but there is still need to be an expert that do the majority of work.
Can build an agent that do a lot but there is still time to have a correct agent that use new ML and not the old way of doing ML.
## Machine learning (AI) business KPIs
![](https://i.imgur.com/ilAogxJ.png)

## Why should AI be related to business KPIs?
Problem with ML is that it is a blackbox. ML better than a linear model as can catch non linear relation in our data and relationship. 
**Explainable AI**: tries to gain trust for ML even from non technical people, guide people through the exploration process for ML. Done through qualitative analytics, use simple qualitative stuff with elementary school Math. Make sense of the complex relationship between data without using ML. Then make a step further and introduce a correlation model. Not just a matter of model but also share methodology. 
Also the adoption of technologies is tied to the monetary value gained(if not much still better to maintain status quo). Can also automate decision creating new jobs and not touching the work of existing people. Easier if you are creating knowledge and not substitute it.
You should have some KPIs tied with ML applications.
## Evaluation of business KPIs
The benefits of AI/machine learning use cases are rarely quantified.
There’s a lack of business benchmarking initiatives. Quantitative evidence almost exclusively comes from suppliers of technology solutions.
For some use cases, economic benefits are difficult or impossible to quantify (KPIs are simultaneously affected by multiple initiatives).
## Why are managers (still) skeptical?
Because AI and machine learning are (and are perceived as) complex.
Because there is no off-the-shelf technical solution.
Because technology is special-purpose and expensive.
Because AI and machine learning are seen as a threat by decision makers (in fact, it may replace some of them).
Because AI and machine learning are associated with the concept of «big data» which adds to their complexity.
A lot of answers derive from technologies application and flops from computer engineers. Usually innovation take in new skills so the people that could be replaced need to be able to pick up these new skills to be desirable for the company, introduce new tasks that are more complex and then simplify the old one.
ChatGPT let know passed the message that AI is complex but its usage is simple and anybody can use it(good approach to market, the product is complex but it is perceived as simple).
A pilot can have positive and negative results, but if there is some continuity there could be some beneficial aspects, but a company when it see a failure stop to invest in the technology without even looking inside the possible reasons and capabilities. 
Data needs to be aligned in time and space for each  feature, problem as the company have different databases, data warehouse and different views for the same object, no fully integrated data. If accuracy is key to the decision you need data quality, so getting high quality data is complex. 
## Why big data? Why now?
ML, LLMs need a lot of data to be trained. 

![](https://i.imgur.com/sHbs3hE.png)

Integration of data is needed for big data as they let the automation of gathering data, the web boosted big data analytics. It created a new application where the marketing people needed to set the parameters of the recommendation system and keep an eye on the recommendation system performances. Need to keep an eye on the ML algorithm as they could make mistakes.
### How big is big data
> Erik Schmidt (Executive Chairman Google): «From the dawn of civilization until 2003, humankind generated 5 Exabytes of data. Now, we are producing 5 exabytes every two days, and the pace is accellerating.»

### What is big data?
- Big data is «a collection of data from traditional and digital sources inside and outside your company that represents a source for ongoing discovery and analysis.» (Source: Forbes)
- Big data is any amount of data that raises technical scalability challenges for a given company due to the increasing growth rate of data and a need for continuous analysis.
Few companies can develop technology for big data so they are limited from what is on the market. From complex analytics I can extract information. Big data and analytics take in consideration complex Math. 
We live under impression that few Terabytes are not big data but when we look at ML algorithm we see that even some Gigabytes are difficult to handle correctly, scalability issues. Need to hit barriers when the competitors hit barriers. 
Two types of scalability
- Technical: the technology can be applied to reach the scope 
- Econiomical: the technology is profitable for the company
## Types of Big data
### Conversation Text Data
Amount of data depends on how big the brand is, small amount of data possible for local brands. 
The problem reside in the semantic engines. You count the resources needed for each brand,
### Image, audio and video data
Can reduce the data with a less resolution image, possible to have on demand analytics creation. 
### Sensor Data
Create data on the collection of the sensors analytics, usually thrown out after some days as they can also create privacy concern.
### The Internet of Things data
IoT produce a vast amount of data.
## Web customer data (Web logs)
Click stream analysis(pre-sales behavioural data). Useful to understand the behaviour of the clients before they buy something(High end of the funnel, they generate big data amount).
## Evolution of big data projects
![](https://i.imgur.com/H9QT64m.png)

At the start no idea of how the data are disposed, if there is seasonality, etc... so you do Descriptive Analytics. DO these analytics to make sense of data. Then you do predictive analytics helps in predicting trends/do better predictions(managers like it more). Then there is Prescriptive analytics if there is a system that can process well the outputs of your models. For example satellites can see the movement of the earth and transmit the possible changes that will happen in the world, can set alarms to see if there are some catastrophes happening.
Innovation is though, a lot of challenges as innovation can be misinterpreted or not trust.
## Main issues with big data projects
1. Getting the technical skills needed to manage the new technologies for big data
2. Getting the data, which are very often stored in multiple databases, not integrated, not ready for analysis (e.g. not structured, not real time)
3. Getting the analytical skills to explore data and gather new and useful insights
4. Achieving business involvement
Consultant company will put labels on their employees even before the employee has the skill, need to be updated on the new things(only impression, maybe there is a small budget on the technology using pilot). Even if a pilot goes wrong the next one will be more successful as the employees have more experience. Usually companies are not in a hurry and are sceptical of your work. Things do not happens quickly, probably need more rounds. Difficulties to decoupling data analytics and economics as they are correlated but, usually, the economic employees use only excels and can see only a fraction of data making wrong decisions.
## Easy and accessible open source: moving from Excel to MySQL
Problem with MySQL is RAM bottleneck, maximum data size is 10 times the amount of data in the cache you can have for the machine.  Usually divide by ten from the row of Excel to have good performance. 
Operational intelligence, need to do things in operations and need data to support them, they have to be detailed at the level of the customer needs(cannot aggregate).
Discover DB and if we apply analytical approaches to the DB we can have good performances, need to use the transaction paradigm. Pay attention to the transaction per second metric as it need to be adjusted to your transactions.
As proprietary SW are expensive there are open sources alternatives. Problem of database is that they caches data, keeping data on disk cost less but is slower and typically there is a factor of 20 between disk space and cache space. This is based on access of most recent data. This can be done in OLTP cases but in OLAP cases this approach is not efficient anymore as you could scan the entire DB to get data.
## Hadoop
Open source technology in DB. Easy to install on single machine, in clustered scenarios it become more complicate. Really have technology competence to be able to install an Hadoop cluster. So there are companies that will distribute the SW but will install for you/guide you to install a cluster(usually marketed as enterprise version of Hadoop), Caldera did an ulterior pass and removed also the need of installation as they will serve the cluster as a cloud service. Commodity HW should not be taken for granted as the company were coming from a period where appliances(SW + specific HW) were common. Now you pay more but the company do not need competences anymore as the technologies become plug and play. 
Built to have horizontal scalability. 5K euros to host an Hadoop server. Remain in the middle of having commodity HW and have it in your data server. Use map-reduce model, write once read many. Needed to adapt to SQL. It has a bad random read performance.
If you want performance you need to denormalize the scheme. Only store data that you REALLY need to do your analytics. 
Difficult to jump to AI in a single leap in a company as there need to be trust.
Getting data is a challenge, company is like a living creature and try to experiment with its technology(usually the rule is if it works do not touch it, do something new otherwise you could be left behind). Need to estimate resources needed for projects, usually they are overestimate and trashed out as they aren't reused in other projects(need to redo) so you need a better management to optimize the procedures, need to have all people work with the same system. 
Need revenues, sales and customer satisfaction to have more infrastructure improvement as you need funds and need to be trusted to be a good source of revenue. 
## Data Lifecycle Management
Usually data are created, updated and never deleted as unused data become an issue only when the storage is full. This is a bad practice, so to be sure data are deleted we need to recognize the deletion of data(also because deleting data is not a priority activity as the business can go on anyway).
Process that cannot fully automated(maybe it can only if it is modelled). Need a process and someone responsible for data management. Someone need to assure that programmer are doing data client, historical data are managed in a different way than normal data.
Key is not read data as it is the most costly operation.
Data schema can be simplified/changed and can be retrieved for when you have to read historical data. 
It is an overhead, need a lot of meetings and time to enforce the policy and to demonstrate that the policy are being applied. 
Another issue is having the right skills. Need to have the vision of where the company needs to go, difficulties in understanding where the technologies are going if they are going to be useful or is more useful to have skilful people. 
Kick the can strategy: postpone/try something and then revoke the decision if it is bad/move decision up in the hierarchy. Analyzing data maybe cannot give an answer but can give a perspective. People need to understand to be committed in analyzing data, business transformation. If you do not have business involved you have problems. 
### Evangelisation
Bringing in new knowledge in a big wave of changes. People in business wished that information technology has reach  maturity, but then new technology bring in new way to do data analytics. 
## BI and analytics platforms
For Gartner Magic Quadrant shows 43 vendors.
Exist a variety of smaller vendors. According to Gartner reporting is the best capability. Vendors can give access to a broader sensor of analytics, demand for something that is not report(analytics). 
Even if we just integrate the access of a company to the market, we reduce the complexity that companies need to execute a business transaction we are adding something, we are letting them save money for the end user company. Usually end user companies have difficulties to handle complexity. Big temptation for the end user companies to simplify their life so they need the aggregators.
Languages create a lock-in. For computer engineer the language should not matter as they know the theoretical concept behind. But when they start to approach a new language professionally they need to pick new libraries and this may cause delay in the deliveries. This is the difference in experience. This is why for a variety of needs we need a variety of competence. This is why the contracts with consultant companies are good as they provide access to these competences without the need to hire permanently new people every time.
Every time you introduce more SW inside your pipeline you are introducing delays but these delay helps you in doing your work easily/faster. If your environment becomes inefficient you need to rollback on the framework and see if it can be substitute with a better one or redo things from scratch. But this simplification comes with the reducing of freedom as you are in the hands of the framework producer. You still need to be like an architect and need to know how something can be done, even with framework you need to know what can be done with each framework.
Over 40 global competitor in Big data and a larger number of local smaller competitor/consulting company.
## Big Data and Business Innovation
Balance make-buy is substained by open source in its make part, but when is buy part companies prefer to buy their solutions. 
Companies have hierarchical mentality, so they do not go for the data driven solution as they are long term organizational changes so they go for quick fixes with no organizational changes. Do not look pragmatically at things/AI only apply it to their needs and very few companies can afford long term views. But this is a issue as of you do not know you cannot plan. 
## The Insourcing Trend
Now the trend is not using other system but develop a SW that represent your company needs(Make instead of buying). There is the problem to know what should be made and what should be bought. Then they need to see what can be made without attacking the legacy SW/HW/Data. To change the legacy system you need to embark on a changing journey that will cost money, but for startup this problem do not arise as they are new and do not have legacy.
Usually the most common approach consist of:
- Buy the infrastructure as it reduce the time required to complete a big data project and limits the loss of technical competence
- Make data management and analyses as they are recognized to be strategic and analytics are tailor-made approach that often provide and edge
### Data Science Group
Need three members:
- Data scientist: they know the business and the data but if there isn't any data they cannot do anything
- Computer scientist: know the SW and HW structure to enable the data analysis
- Data Manager: assure that data are there and they are correct
Pay attention to not do choices only based on market because if so you can become useless.
## How to manage a big data project
1. Define your use cases, do not choose anything that incorporate great organizational changes, attract people, start with one or two cases with clear KPIs
2. Determine the project team, choose the people in your organization or hire someone(best if the data scientist is inside the company already)
3. Plan your project
Start from more traditional approaches and do qualitative approaches, look at the data with the lenses of your own company. This is the step where you build trust on data. Data scientist will tell company something that the company already know, and this step is useful as it confirms the company data. Explainable AI(algorithm as a black box), people find trust issues on the results so explainable AI is the approach to gain the trust for the results thanks to letting users know way to crosscheck data. Also need to know how we get the data, DO NOT assume that you have all the data that you need and this is an effort that needs to be done. One issue is data quality and a problem is to not be stuck on it as this will lead to not progress the project. So you need to have a good enough quality on your dataset for the task you need to do. 
Then you need to find a sponsor for your project. Need to get someone that trust you and so you can go on with the project, not having support can lead to technical debt that, if not cured, can lead to problems and the failure of your organization.
You need KPIs to understand how well the project is going, need to asses the rules ahead of time, need to base the rules before the results to not have biased rules.
Define the technical requirements. In many cases the company need to rely on suppliers to gain some skills. 