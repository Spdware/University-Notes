# Introduction
In centralized applications we cares about bounds as faults are handled by the program and are really clear to the users. In a distributed system the faults are partial and can or cannot be seen to the users. 
Building dependable systems:
- Availability: We want the system to be ready for use as soon as we need it. 
- Reliability: The system should be able to run continuously for long time
- Maintainability: It should be easy to fix. This don't have a direct impact with the others property but helps to improve them.
- Safety: It should cause nothing bad to happen. This can also include the fact that you don't loose data if the system crashes.
Reliability and Availability are related but they aren't the same thing.
A system **fails** when it is not able provide its services. A **failure** is the result of an error. An error is **caused** by a fault. Some faults can be avoided, others cannot.
Building a dependable system demands the ability to deal with the presence of faults
A system is said to be fault tolerant if it can provide its services even in the presence of faults
Faults can be classified according to the frequency at which they occur
- Transient faults occur once and disappear
- Intermittent faults appear and vanish with no apparent reason
- Permanent faults continue to exist until the failed components are repaired
### Faults/Failures in DS
Different types of failures (failure model):
- Omission failures:Processes: fail-safe (sometimes wrong, but easily detectable output), fail-stop (detectable crash), fail-silent (undetectable crash). Channels: send omission, channel omission, receive omission
- Timing failures (apply to synchronous systems, only): Occur when one of the time limits defined for the system is violated
- Byzantine (or arbitrary) failures: Processes: may omit intended processing steps or add more. Channels: message content may be corrupted, non-existent messages may be delivered, or real messages may be delivered more than once
### General techniques
- Information redundancy: we replicate information to avoid the damage caused by a fault. Add information to cope with the failure
- Time redundancy: redo an operation if you don't see its results
- Physical redundancy: replicate the system to cope with failure of the system
Typical biologic system use physical redundancy(EX: a physical component that generates data and you have more copy of this component doing the same things.Then the data are passed to a voter that calculate which data is more probable and discard the other data. This operation is repeated multiple time)
### Clients crash too
A computation started by a dead client is called an orphan. Orphans still consume resources on the server. The server must get rid of them:
- Extermination: RPC’s are logged by the client and orphans killed (on client request) after a reboot -> costly (logging each call) and problem with grand orphans and network partitions
- Reincaration: when a clients reboots it starts a new epoch and sends a broadcast message to servers, who kill old computations started on behalf of that client (including grand orphans). Replies sent by orphans bring an obsolete epoch and can be easily discarded
- Gentle reincarnation: as before but servers kill old computations only if owner cannot be located
- Expiration: Remote computation expire after some time. Clients wait to reboot to let remote computations to expire
Reincarnation may seem to solve all problems, but what if the killed orphans had open files? There’s a lot of bookeeping involved, and not all issues can be solved
# Protection against process failures: Agreement in process groups
### Process Failures
The easiest approach is to introduce redundancy. Redundancy can be used to mask the presence of faulty processes, with redundant process groups: the work that should be done by a process is taken care by a group of processes and the healthy processes can continue to work when some of the others fail

![](https://i.imgur.com/sJzJftV.png)

There are details that we didn't consider:
- We need to keep track of which processes constitute each group.
- We need to have join and leave announcements to be reliably multicast
How large should a group be? In general of the type of failures that you want to tolerate are omission failures in processes to be k-fault-tolerant you need k+1 processes. If you have Byzantine failures things become worse as you need 2k+1 processes to have a k-fault-tolerance(to have a working voting mechanism)
### Agreement in process groups
We want the components of a group to agree on something as who is the leader, the client request is satisfied, which value to send to the client...
This is different from before as the decision come from the components of the system. 
**The consensus problem**
A set of processes must agree on some value: the non-faulty ones must agree
The value is subject to a validity condition: It cannot be any value
More precisely, we have the following conditions
- Each process starts with some initial value
- All non-faulty processes have to reach a decision based on these initial values
- The following properties must hold
	- Agreement: No two processes decide on different values
	- Validity: If all processes start with the same value v, then v is the only possible decision value
	- Termination: All non-faulty processes eventually decide
### Consensus with process failures
The coordinated attack problem shows that consensus is not possible in the presence of arbitrary communication failures
**Assumptions for crashing processes**
Let’s review our assumptions:
- We consider a synchronous system, in which all processes evolve in synchronous rounds
	- A message sent by a process is received within the same round by the recipient (or within a bounded number of rounds)
	- Processes may fail at any point, by stopping taking steps (fail-silent)
-  We want our processes to reach agreement according to the previous definition of consensus
The problem we just stated is, luckily, easier than the previous one: it turns out that the problem can be solved provided that the processes take at least f+1 rounds with f being a bound on the number of failures. Moreover, it is sufficient that a single process be non-faulty
### FloodSet algorithm
Let us now consider a simple algorithm to solve the problem: 
Let v0 be a pre-specified default value
Each process maintains a variable W (subset of V) initialized with its start value
The following steps are repeated for f+1 rounds:
- Each process sends W to all other processes
- It adds the received sets to W
The decision is made after f+1 rounds
- if |W| = 1: decide on W's element
- if |W| > 1: decide on v0 (or use the same function to decide, e.g. max(W))
Remember: each process may stop in the middle of the send operation
**Prooving FloodSet correcteness**
_Lemma_
If no process fails during a particular round r : 1 ≤ r ≤ f+1, then Wi(r) = Wj(r) for all i and j that are active after r rounds
Suppose that Wi(r) = Wj(r) for all i and j that are active after r rounds. Then for any round r1 : r ≤ r1 ≤ f+1, the same holds, that is, Wi(r1) = Wj(r1) for all i and j that are active after r1 rounds
- If processes i and j are both active after f+1 rounds, then Wi = Wj at the end of round f+1
When we have Byzantine failures things complicates(In realities Byzantine failures don't happens often and to avoid them and usually there are crashes the system, usually it is during an attack that you have a Byzantine failures. They happen usually in channels and they are easy to detect and correct). 
Our problem is now defined as follows:
- Agreement: No two non-faulty processes decide on different values
- Validity: If all non-faulty processes start with the same value v, then v is the only possible decision value for all non-faulty processes
- Termination: All non-faulty processes eventually decide
The idea is that we cannot guarantee anything about faulty processes
So far we have considered synchronous systems. A real distributed system is inherently asynchronous. Let us consider the problem of reaching an agreement between n processes with 1 faulty process in an asynchronous system. Let’s start with the “easy case”: Fail-silent failures: 
Fischer, Lynch, and Paterson proved that no solution exists: “Impossibility of Distributed Consensus with One Faulty Process” (FLP Theorem)
- The result was proved in the case of crash failures
- What happens in the case of Byzantine failures?A byzantine general can simulate a crashed general so if a solution existed for byzantine failures that solution would also work for crash failures
# Reliable group communication
It is of critical importance if we want to exploit process resilience by replication
Achieving reliable multicast through multiple reliable point-to-point channels may not be efficient
or enough. What happens if the sender crashes while sending? What if the group changes while a message is being sent?
Groups are fixed, and processes non-faulty
-  All group members should receive the multicast (not necessarily in the same order)
- Easy to implement on top of unreliable multicast
	- Positive acknowledgements: large number of ack messages
- Negative acknowledgments: fewer messages but sender has to cache messages
Case of faulty processes (we will see it later):
- All non-faulty group members should receive the message
- There must be agreement about who is in the group
### Non-faulty processes: Basic approach
![](https://i.imgur.com/GiWYxuf.png)

### Scalable Reliable Multicast
A first solution is non-hierarchical feedback control:
- Negative acknowledgements are multicast, good to suppress replicated NACK but also bad since everyone must process NACKs

![](https://i.imgur.com/Uj5CfMJ.png)

To use a NACK we need a way to know we lost a packet. We give the packets a number and if we don't have all the ID we can send a NACK. The advantages is that the network is reliable and we don't need to send NACKs often BUT the problem is that if something goes wrong instead of immediatly sending a NACK you wait for a random time and look around the channel. When the timer end you send in broadcast the NACK message.
Works when the group is flat
### Hierarchical Feedback COntrol
![](https://i.imgur.com/3zrJpJq.png)

With Hierarchical Feedback control, receivers are organized in groups headed by a coordinator and groups are organized in a tree routed at the sender. You use Scalable Multicast in the group and then elect a leader that forward the messages between groups.
The coordinator can adopt any strategy within its group. In addition it can request retransmissions to its parent coordinator
A coordinator can remove a message from its buffer if it has received an ack from:
- All the receivers in its group
- All of its child coordinators
The problem is that the hierarchy (tree) has to be constructed and maintained. 
### The case of faulty processes
If the sender process crashes, and in particular during transmission, there is the problem that the message could be sent to a portion of the receivers. If this issue don't interleave with the fact that the crash occurs.
Things get harder if processes can fail or join and leave the groups during communication. What is usually needed is that a message be delivered either to all the members of a group or to none, and that the order of messages be the same at all receivers. This is known as the atomic multicast problem. 
Ideally we would like:
-  Any two processes that receive the same multicast messages or observe the same group membership changes to see the corresponding events in the same order
- A multicast to a process group to be delivered to its full membership. The send and delivery events should be considered to occur as a single, instantaneous event
Unfortunately, close synchrony cannot be achieved in the presence of failures
We can obtain virtual synchrony: the message is delivered either to all of the member in the group or to none and it will be delivered before telling that the sender has crashed(We cannot receive messages after we know that the sender crashed).

![](https://i.imgur.com/WfOvrWK.png)

You must order and synchronize messaging with the information of crashed processes. You can do it at the middle layer

![](https://i.imgur.com/iThwFwy.png)

We say that a view change occurs when a process joins or leaves the group, possibly crashing
All multicast must take place between view changes:
- We can see view changes as another form of multicast messages (those announcing changes in the group membership)
- We must guarantee that messages are always delivered before or after a view change
- If the view change is the result of the sender of a message m leaving the message is either delivered to all group members before the view change is announced or it is dropped
Multicasts take place in epochs separated by group membership changes
A multicast system can be reliable and unordered.
Retaining the virtual synchrony property, we can identify different orderings for the multicast messages
- Unordered multicasts
- FIFO-ordered multicasts
- Causally-ordered multicasts
In addition, the above orderings can be combined with a total ordering requirement(whatever ordering is chosen, messages must be delivered to every group member in the same order)
Virtual synchrony includes this property in its own definition
### Virtual Synchrony
A group view is the set of processes to which a message should be delivered as seen by the sender at sending time. The minimal ordering requirement is that group view changes be delivered in a consistent order with respect to other multicasts and with respect to each other
This, together with the previous requirements, leads to a form of reliable multicast which is said to be virtually synchronous
We say that a view change occurs when a process joins or leaves the group, possibly crashing
All multicast must take place between view changes:
- We can see view changes as another form of multicast messages (those announcing changes in the group membership)
- We must guarantee that messages are always delivered before or after a view change
- If the view change is the result of the sender of a message m leaving the message is either delivered to all group members before the view change is announced or it is dropped
Multicasts take place in epochs separated by group membership changes
### Atomic multicast
Atomic is a synonimous of totally ordered. Atomic multicast is defined as a virtually synchronous reliable multicast offering totally-ordered delivery of messages

![](https://i.imgur.com/bD3rSa3.png)

Virtual Synchrony is the best we can do.